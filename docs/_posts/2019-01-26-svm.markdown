---
layout: post
title:  "机器学习 - Support Vector Machines"
date:   2019-01-26 16:43:01 +0800
categories: machine-learning
---

本篇博文主要是使用python的scikit-learn库重新实现吴恩达机器学习课程的
程序设计6的练习。该练习分两个部门，第一部分通过几个2D的数据集来了解SVMs时如何工作的和如何使用高斯内核，而第二部门则是使用SVM构建一个垃圾分类器。

## Dataset1 的练习

第一个练习的数据集是可线性区分的。

首先，在octave中导出数据：
```octave
load("ex6data1.mat")
csvwrite("ex6data1.csv",[X,y])
```

接着，用pandas库读取csv数据。

```python
import pandas as pd
ex6data1 = pd.read_csv('ex6data1.csv',header=None)
print(ex6data1.head())
```

训练数据的前五行如下：

            0       1  2
    0  1.9643  4.5957  1
    1  2.2753  3.8589  1
    2  2.9781  4.5651  1
    3  2.9320  3.5519  1
    4  3.5772  2.8560  1

### 数据可视化

分离出 positive 和 negative 数据。

```python
positive_train_examples = ex6data1[ex6data1[2] == 1];
negative_train_examples = ex6data1[ex6data1[2] == 0];
```

用 matplotlib 库绘制该数据集，positive 数据用黑色的 + 表示，
而 negative 数据则用黄色的 o 表示。

```python
from matplotlib import pyplot as plt

def plotData(px,py,nx,ny,title):
    plt.plot(px,py,'k+',nx,ny,'yo')
    plt.title(title)
    plt.show()
    
plotData(positive_train_examples[0],positive_train_examples[1],
         negative_train_examples[0],negative_train_examples[1],
        "Figure 1: Example Dataset 1")
```


![Example Dataset 1]({{ "assets/images/machine-learning-ex6/svm_dataset_1/output_2_0.png" | relative_url }} )

由图可知，positive 数据基本位于图片的右上角，而 negative 数据则位于左下角。需要特别指出的是，有一个 outlier positive 训练数据，不在右上角，而时在左上角，大概是在（0.1，4.1）的位置。

在这个练习中，主要是了解 outlier 数据如何影响 SVM 的 decision boudary。

### 训练模型

从 sklearn 库中导入 svm 模块，使用其中 SVC 模型，并将其内核设置为 *linear*。

```python
from sklearn import svm
X = ex6data1.as_matrix([0,1])
y = ex6data1.as_matrix([2])

svm_model = svm.SVC(kernel="linear")
svm_reg = svm_model.fit(X,y)

print(svm_reg.coef_,svm_reg.intercept_)
```

模型的参数表示如下：

    [[1.40718563 2.13398052]] [-10.34889778]

### 训练模型曲线化

由上可知，该svm的数学模型为：

> x1 * 1.41 + x2 * 2.134 - 10.35 = 0
> 
> x2 = - (x1 * 1.41 + (-10.35)) / 2.134
> 
> x2 = - (x1 * w[0] + b) / w[2]
> 
> w = coef_[0], b = intercept_

```python
import numpy as np

def visualBoundaryLinear(X,y,model,title):
    xp = np.linspace(min(X[:,0]),max(X[:,0]),100)
    w = model.coef_[0]
    b = model.intercept_
    y = - (w[0] * xp + b) / w[1]
    plt.plot(xp,y,'b-')
    plotData(positive_train_examples[0],positive_train_examples[1],
         negative_train_examples[0],negative_train_examples[1],
         title)

visualBoundaryLinear(X,y,svm_model,"SVM Decision Boundary with C = 1 (Example Dataset 1)")
```

![SVM Decision Boundary with C =1 ]({{ "assets/images/machine-learning-ex6/svm_dataset_1/output_5_0.png" | relative_url}})

由图可知，outlier被模型预测为了 negative 类别。接下来，将通过参数 C 来微调SVM模型。

## Tune the Model

> In this part of the exercise, you will try using different values
> of the C parameter with SVMs. Informally, the C parameter is a 
> positive value that controls the penalty for misclassified training
> examples.

> A large C parameter tells the SVM to try to classify all the examples corretcly.

设置参数C=100，再次训练模型。

```python
svm_model_c100 = svm.SVC(kernel="linear", C=100)
svm_reg_c100 = svm_model_c100.fit(X,y)
visualBoundaryLinear(X,y,svm_model_c100,"SVM Decision Boundary with C = 100 (Example Dataset 1)")
```

![SVM Decision Boundary with C = 100]({{ "assets/images/machine-learning-ex6/svm_dataset_1/output_6_1.png" | relative_url }})

由图可知，当参数C为100时，outlier已经被正确的归类为了 positive 类别了。

## SVM with Gaussian Kernels

这部分会使用高斯内核的SVM模型来训练非线性的模型分类器。

### 高斯内核

> Gaussian kernel: a similarity function that measures the 
> "distance" between a pair of examples,(xi,xj).

```python
import numpy as np
def gaussian_kernel(x1,x2,sigma):
    return np.exp(-sum((x1-x2)**2) / (2*sigma**2))

x1 = np.array([1,2,1])
x2 = np.array([0,4,-1])
sigma = 2

sim = gaussian_kernel(x1,x2,sigma)
print(sim)
```

    0.32465246735834974

### Example Dataset 2

用pandas的api，read_csv加载从octave中导出的数据，ex6data2.csv.

```python
import pandas as pd

data = pd.read_csv('ex6data2.csv',header=None)
print(data.head())
```

              0         1  2
    0  0.107143  0.603070  1
    1  0.093318  0.649854  1
    2  0.097926  0.705409  1
    3  0.155530  0.784357  1
    4  0.210829  0.866228  1

接着，可视化 Example Dataset 2数据集。

```python
from matplotlib import pyplot as plt
def plotData(data,title):
    positive_examples = data[data[2]==1]
    negative_examples = data[data[2]==0]
    
    plt.plot(positive_examples[0],positive_examples[1], 'k+',
             negative_examples[0],negative_examples[1], 'yo')
    plt.title(title)
    plt.show()
    
plotData(data,"Figure 4: Example Dataset 2")
```

![Figure 4: Example Dataset 2]({{ "assets/images/machine-learning-ex6/svm_with_gaussian_kernels/output_3_0.png" | relative_url}})

### 训练模型

sklearn中的SVM模型，没有用到 sigma 参数，但有个类似的参数 gamma。
由[文档svm kernels][svm kernels]中，可知：
> gamma = 1.0 / (2 * sigma * sigam)

```python
from sklearn import svm

X = data.as_matrix([0,1])
y = data.as_matrix([2])
# https://scikit-learn.org/stable/modules/svm.html#svm-kernels
sigma = 0.1
gamma = 1 / (2*sigma**2)
print('gamma is: ', gamma)
model = svm.SVC(kernel='rbf',gamma=gamma)
reg = model.fit(X,y)
```

    gamma is:  49.99999999999999

### 模型可视化

利用contour函数，可以可视化出模型的 Decision Boundary。

```python
# https://scikit-learn.org/stable/auto_examples/svm/plot_iris.html
def make_meshgrid(x,y,h=0.02):
    x_min, x_max = x.min(), x.max()
    y_min, y_max = y.min(), y.max()
    xx, yy = np.meshgrid(np.arange(x_min,x_max,h),
                         np.arange(y_min,y_max,h))
    return xx,yy

def plot_contours(clf, xx, yy):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx,yy,Z,colors='b', linewidths=1)
    plotData(data,'Figure 5: SVM (Gaussian Kernel) Decision Boundary')
    
X0,X1 = X[:,0],X[:,1]
xx, yy = make_meshgrid(X0,X1)

plot_contours(reg,xx,yy)
```

![Firgure 5: SVM (Gaussian Kernel) Decision Boundary]({{ "assets/images/machine-learning-ex6/svm_with_gaussian_kernels/output_4_0.png" | relative_url}})

[svm kernels]:https://scikit-learn.org/stable/modules/svm.html#svm-kernels

## Example Dataset 3

和前面两个数据集一样，通过 csvwrite 接口导出数据为 ex6data3_training.csv 和
ex6data3_cv.csv 两个数据集。一个用来训练的，另一个则是用来交叉验证。

```python
import pandas as pd
training_data = pd.read_csv('ex6data3_training.csv',header=None)
cv_data = pd.read_csv('ex6data3_cv.csv',header=None)
print(training_data.head())
print(cv_data.head())
```

              0         1  2
    0 -0.158986  0.423977  1
    1 -0.347926  0.470760  1
    2 -0.504608  0.353801  1
    3 -0.596774  0.114035  1
    4 -0.518433 -0.172515  1
              0         1  2
    0 -0.353062 -0.673902  0
    1 -0.227126  0.447320  1
    2  0.092898 -0.753524  0
    3  0.148243 -0.718473  0
    4 -0.001512  0.162928  0

接着，因为有两份数据可以可视化。因此先抽象一个简单的可视化函数。

```python
from matplotlib import pyplot as plt

def plotData(data,title):
    positive_examples = data[data[2]==1]
    negative_examples = data[data[2]==0]
    
    plt.plot(positive_examples[0],positive_examples[1],'k+',
             negative_examples[0],negative_examples[1],'yo')
    plt.title(title)
    plt.show()
```

对训练数据的可视化。

```python
# visualizing the trainning data
plotData(training_data,"Figure 6: Example Dataset 3")
```

![Example Dataset 3]({{ "assets/images/machine-learning-ex6/svm_dataset_3/output_2_0.png" | relative_url}})

对交叉验证数据集可视化。

```python
# visualizing the cross validation data
plotData(cv_data, "Example Dataset 3 (cross validation)")
```

![Example Dataset 3 (cross validation)]({{ "assets/images/machine-learning-ex6/svm_dataset_3/output_3_0.png" | relative_url}})

Dataset 3主要是通过交叉验证数据集来筛选出误差率相对比较低的C和simga参数。
大体的思路是，对C和simga取一定范围的数值，然后构建训练模型，用交叉验证数据去获取误差值，取其中误差值最低的那一对C和sigma参数。

```python
from sklearn import svm
import numpy as np

X = training_data.as_matrix([0,1])
y = training_data[2]
Xval = cv_data.as_matrix([0,1])
yval = cv_data[2]

def calc_prediction_error(y_predictions, y):
    return np.mean(y_predictions!= y)

params = [0.01,0.03,0.1,0.3,1,3,10,30]
C = 0
sigma = 0
cmax_error = 1000000

for ci in params:
    for si in params:
        gamma = 1.0/(2.0*si*si)
        
        svm_model = svm.SVC(C=ci,gamma=gamma)
        svm_reg = svm_model.fit(X,y)

        y_predict = svm_reg.predict(Xval)

        predictions_error = calc_prediction_error(y_predict, yval)
        
        # print(ci, si,gamma, predictions_error)
        
        if predictions_error < cmax_error:
            C = ci
            sigma = si
            cmax_error = predictions_error

print(C,sigma,cmax_error)
```

    1 0.1 0.035

由输出可知，当C=1和sigma=0.1时，误差相对较小，为0.035.

最后，通过上述的C和simga参数对，重新训练模型，并可视化模型的Decision Boundary。

```python
def make_meshgrid(x,y,h=0.02):
    x_min, x_max = x.min(), x.max()
    y_min, y_max = y.min(), y.max()
    xx, yy = np.meshgrid(np.arange(x_min,x_max,h),
                         np.arange(y_min,y_max,h))
    return xx,yy

def plot_contours(clf, xx, yy,title):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx,yy,Z,colors='b', linewidths=1)
    plotData(training_data,title)
    
X0,X1 = X[:,0],X[:,1]
xx, yy = make_meshgrid(X0,X1)

gamma = 1.0 / (2*sigma*sigma)
svm_model = svm.SVC(C=C,gamma=gamma)
reg = svm_model.fit(X,y)

plot_contours(reg,xx,yy,"SVM (Gaussian Kernel) Decision Boundary (Example Dataset 3)")
```

![SVM (Gaussian Kernel) Decision Boundary (Example Dataset 3)]({{ "assets/images/machine-learning-ex6/svm_dataset_3/output_5_0.png" | relative_url}})

## Spam Classification

从vocab.txt获取特征word。

```python
def getVocabList():
    vocabList = []
    with open('vocab.txt','r') as fp:
        vocabList = fp.read().split('\n')
    return vocabList[:-1]

vocabDict = {}
vocabList = getVocabList();
for vocab in vocabList:
    line = vocab.split('\t')
    vocabDict[line[1]] = int(line[0])   
```

从邮件的内容中提取特征。

```python
import re
import numpy

def processEmail(email_contents):
    email_contents = email_contents.lower()
    email_contents = re.sub(r'<[^<>]+>', '', email_contents)
    email_contents = re.sub(r'[0-9]+','number',email_contents)
    email_contents = re.sub(r'(http|https)://[^\s]*','httpaddr',email_contents)
    email_contents = re.sub(r'[^\s]+@[^\s]+','emailaddr', email_contents)
    email_contents = re.sub(r'[$]+','dollar',email_contents)
    
    email_contents = re.sub(r'[ @$/#\.\-:&\*\+=\[\]\?\!\(\)\{\},\'\'">_<;%]', ' ', email_contents)
    email_contents = re.sub(r'\n', ' ', email_contents)
    words = email_contents.split(' ')
    words = [word for word in words if word != '']

    word_indices = np.zeros(len(vocabDict))
    
    for word in words:
        if word in vocabDict.keys():
            # print(word,vocabDict[word])
            word_indices[vocabDict[word]] = 1
    return word_indices
    
with open('emailSample1.txt', 'r') as fp:
    email_contents = fp.read()
    email_contents = processEmail(email_contents)
    print(email_contents)
```

    [0. 0. 0. ... 1. 0. 0.]

加载 spamTrain.csv 训练数据，用sklearn中的线性SVC模型训练，经计算可知，
该模型的准确率为 0.998.

```python
spam_train_data = pd.read_csv('spamTrain.csv',header=None)
```


```python
from sklearn import svm
X = spam_train_data.as_matrix(np.arange(0,1898))
y = spam_train_data[1899]

svm_model = svm.SVC(kernel='linear',C=0.1)
svm_reg = svm_model.fit(X,y)

print(svm_reg.score(X,y))
```

    0.99825

加载测试数据，根据上述训练模型的预测的准确率为：0.989.

```python
spam_test_data = pd.read_csv('spamTest.csv',header=None)
Xtest = spam_test_data.as_matrix(np.arange(0,1898))
ytest = spam_test_data[1899]
print(svm_reg.score(Xtest,ytest))
```

    0.989

## 小结

* 了解了 scikit-learn 的 svm 模块
* svm.SVC(kernel='linear') 来处理线性分类器
* svm.SVC(kernel='rbf') 来处理非线性分类器
* 了解了参数 C 和 gamma 如何影响分类器的准确性
* 了解countour来显示 svm 的 decision boundary